# memo
The memory bank component developed for playing virtual roles in a specific scenario for large-scale models.

20240316
在我近期的测试中，常常出现输入问题后，关键词匹配记忆为空的情况。
这个问题有两方面的因素，一方面是因为我的数据量（3000条左右）不够大，另一方面也和匹配策略不够好有关。

这里主要讨论下第二个方面，匹配策略。
在我的视频中的组件中，匹配记忆的因素有两个：记忆相似度和记忆遗忘程度。
问题就出在记忆相似度上——单论记忆相似度匹配这个角度来说，向量嵌入模型或许是更好的选择。

诚然，用关键词匹配+近义词组模糊匹配的思路来寻找相似记忆，速度极快（因为相似度事先已经计算好，存在了数据库中，只需要进行一次keybert关键词提取就可以）。
但是在本应用场景，即数千条左右小规模数据上，向量嵌入模型的速度也很快。
在这种情况下，为了少许效率而影响匹配结果，显然不是好做法。

至于为什么有时会出现关键词匹配记忆为空的情况，我认为和我的阈值设置有关。
在记忆库的实际设置中，我的阈值设得比较高，可以理解为只记录了相似度较高的记忆的相似关系，在泛用性上效果不如向量嵌入模型。
较多数据的场景下，或许这个问题不会暴露得太明显，但数据较少的时候就不合适了。

我个人认为，从原理上讲两种方案有相似的地方。
bert和向量嵌入模型基本架构都来源于transformer，预训练过程中也都可以把握整体语义。
但向量嵌入模型因为大模型出现后许多公司和开源作者做的新工作，在这个任务上更高效。
从实践角度来说，比起闭门造车，尝试用老模型解决新问题，或许积极地阅读和了解新成果才是更好的做法。
我之前确实犯了一些错误，没有积极地关注最新的开源解决成果，而是想着靠已有的NLP知识解决问题，导致走了一些弯路。希望没有给关注的大家带来误导。

在后续的工作中，我会尝试引入开源的向量嵌入模型，与已有方案进行效率对比。
同时，对于已有方案（keybert+近义词组模糊匹配），我会尝试调整阈值，看看能否取得更好效果。
但毕竟分析归分析，实践归实践。已有的向量嵌入模型，考虑的场景更多的还是海量数据的知识库，或许并不是在这个任务上的最优解？
无论如何，在阅读了最新的文章后，我还是倾向于向量嵌入模型能有更好的效果。
而且向量嵌入模型的训练代价似乎也不大，如果已有的向量嵌入模型在本情景下表现不佳的话，我会试试能否通过调整架构，训练一个针对性的模型。
但最近确实没太多时间弄，如果有人还在关注这个项目，只能请你多期待一段时间了。

origin
本组件为大模型扮演虚拟角色这一特定情境开发的记忆库组件。
其基本逻辑是构建角色四维记忆体系架构，同时不断学习用户和大模型之间的对话，生成新记忆，筛选后将其存入记忆库中。
在用户输入新信息时，记忆库内部会进行检索，搜寻相似的对话记录，并提取到prompt中辅助大模型输出。

提取对话记录一方面可以起到稳定角色发言风格、习惯的作用，另一方面可以起到提供身份信息的作用。任何熟悉大模型工作的人都会知道这是很有效的。
在虚拟角色扮演的特殊情境下，由于单次发言的tokens普遍较短，正好为提取对话记录提供了输入空间。

组件的基本逻辑是构建角色四维记忆体系架构，确定记忆的可靠度，同时不断学习用户和大模型之间的对话，生成新记忆，经bert模型筛选后将其存入记忆库中。
记忆库内部建立倒排索引，通过关键词去定位相应记忆，提取到prompt中辅助大模型输出。
记忆库内部通过记忆遗忘、关键词概率、词语相似度等机制，为记忆匹配确定优先级。

下面逐一说明代码功能
check_similar:相似度模型部署，简单的bert二分类微调模型
check_style：角色风格/身份信息筛选模型部署，基于特定角色发言（正样本）和其他角色发言（负样本）训练的bert二分类模型，正样本来自下文的基础记忆和核心记忆
clean：内部去重，通过相似度bert模型删除倒排索引中语义相同的记忆
get_keyword：keybert模型部署
get_simi_word：word2vec模型部署，对于某关键词，得到它的近义词群，作为倒排索引的key
get_temp_base：基础记忆生成临时JSON文件，基于角色原作品高质量对话台词，千条左右
get_temp_core：核心记忆生成临时JSON文件，基于self_config标记，百条左右
initiate：一键完成前期模型和数据库部署
main：完成初始化后，程序入口，输入用户发言，返回记忆提取结果
model_test：测试各个模型
write：将JSON临时文件内容写入sqlite数据库

程序基于torch 1.11.0 cuda 11.3 开发 如果你的torch版本超过2 那么应该是用不了torch的

想要使用这个工具，你需要做的事：
1.收集特定角色的核心记忆、基础记忆，然后利用get_temp_base、get_temp_core将其生成符合项目要求的JSON形式
2.基于这些数据，训练特定角色的风格/身份信息筛选模型，以保证你数据库中的数据质量
3.看我的drive文件夹下，将空白的模型全部部署，具体可以查看里面的readme文档
4.在database中建立一个停用词文档，以删除倒排索引中你不需要的关键词

或者，可以稍等一段时间，我会尽快完善现有的问题
学习记忆功能已经开发完成，但学习记忆需要大模型适配才能得到输出，我会在测试完毕后尽快提交相关代码
更具体的技术说明 可以去b站搜索“魔女多萝茜” 有视频讲解 希望能对你有帮助！ 希望和你交流并期待你宝贵的建议
